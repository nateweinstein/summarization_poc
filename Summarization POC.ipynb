{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9837123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df0448eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Login successful\n",
      "Your token has been saved to /Users/nateweinstein/.huggingface/token\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49a2814b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset billsum (/Users/nateweinstein/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'truncate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      4\u001b[0m billsum \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbillsum\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mca_test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mbillsum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncate\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'truncate'"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/tasks/summarization\n",
    "# import transformers\n",
    "from datasets import load_dataset\n",
    "billsum = load_dataset(\"billsum\", split=\"ca_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0799b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = billsum.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(billsum['train'][2])\n",
    "# import pandas as pd\n",
    "# data = pd.DataFrame(data=billsum['train'])\n",
    "# data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59024a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nateweinstein/opt/miniconda3/envs/summarization_poc/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2890a47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/nateweinstein/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc/cache-1c80317fa3b1799d.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/nateweinstein/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc/cache-bdd640fb06671ad1.arrow\n"
     ]
    }
   ],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=850, truncation=True)\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_billsum = billsum.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bacd71",
   "metadata": {},
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c3d3755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff642fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76ee0613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/nateweinstein/.cache/huggingface/hub/models--t5-small/snapshots/3479082dc36f8a4730936ef1c9b88cd8b0835c53/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/nateweinstein/.cache/huggingface/hub/models--t5-small/snapshots/3479082dc36f8a4730936ef1c9b88cd8b0835c53/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "480145a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nateweinstein/Projects/hugging_face/intro/my_awesome_billsum_model is already a clone of https://huggingface.co/natedog/my_awesome_billsum_model. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "WARNING:huggingface_hub.repository:/Users/nateweinstein/Projects/hugging_face/intro/my_awesome_billsum_model is already a clone of https://huggingface.co/natedog/my_awesome_billsum_model. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/Users/nateweinstein/opt/miniconda3/envs/summarization_poc/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 989\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 248\n",
      "  Number of trainable parameters = 60506624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='248' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [248/248 3:11:02, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.811071</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.107300</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.598755</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.534993</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.111900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.517903</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.112900</td>\n",
       "      <td>0.112900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 248\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 248\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 248\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 248\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=248, training_loss=3.0268109229303177, metrics={'train_runtime': 11488.9092, 'train_samples_per_second': 0.344, 'train_steps_per_second': 0.022, 'total_flos': 1070824333246464.0, 'train_loss': 3.0268109229303177, 'epoch': 4.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_awesome_billsum_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "#     fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38183492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to my_awesome_billsum_model\n",
      "Configuration saved in my_awesome_billsum_model/config.json\n",
      "Model weights saved in my_awesome_billsum_model/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_billsum_model/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_billsum_model/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (3) will be pushed upstream.\n",
      "WARNING:huggingface_hub.repository:Several commits (3) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n",
      "WARNING:huggingface_hub.repository:The progress bars may be unreliable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch response: Authorization error.\n",
      "error: failed to push some refs to 'https://huggingface.co/natedog/my_awesome_billsum_model'\n",
      "\n",
      "WARNING:huggingface_hub.repository:batch response: Authorization error.\n",
      "error: failed to push some refs to 'https://huggingface.co/natedog/my_awesome_billsum_model'\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "batch response: Authorization error.\nerror: failed to push some refs to 'https://huggingface.co/natedog/my_awesome_billsum_model'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/summarization_poc/lib/python3.9/site-packages/huggingface_hub/repository.py:1213\u001b[0m, in \u001b[0;36mRepository.git_push\u001b[0;34m(self, upstream, blocking, auto_lfs_prune)\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m return_code:\n\u001b[0;32m-> 1213\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError(\n\u001b[1;32m   1214\u001b[0m                     return_code, process\u001b[38;5;241m.\u001b[39margs, output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr\n\u001b[1;32m   1215\u001b[0m                 )\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['git', 'push', '--set-upstream', 'origin', 'main']' returned non-zero exit status 1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/summarization_poc/lib/python3.9/site-packages/transformers/trainer.py:3452\u001b[0m, in \u001b[0;36mTrainer.push_to_hub\u001b[0;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   3449\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush_in_progress\u001b[38;5;241m.\u001b[39m_process\u001b[38;5;241m.\u001b[39mkill()\n\u001b[1;32m   3450\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush_in_progress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3452\u001b[0m git_head_commit_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_lfs_prune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   3454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3455\u001b[0m \u001b[38;5;66;03m# push separately the model card to be independant from the rest of the model\u001b[39;00m\n\u001b[1;32m   3456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/summarization_poc/lib/python3.9/site-packages/huggingface_hub/repository.py:1438\u001b[0m, in \u001b[0;36mRepository.push_to_hub\u001b[0;34m(self, commit_message, blocking, clean_ok, auto_lfs_prune)\u001b[0m\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgit_add(auto_lfs_track\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgit_commit(commit_message)\n\u001b[0;32m-> 1438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgit_push\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morigin \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_branch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_lfs_prune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_lfs_prune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1442\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/summarization_poc/lib/python3.9/site-packages/huggingface_hub/repository.py:1218\u001b[0m, in \u001b[0;36mRepository.git_push\u001b[0;34m(self, upstream, blocking, auto_lfs_prune)\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError(\n\u001b[1;32m   1214\u001b[0m                     return_code, process\u001b[38;5;241m.\u001b[39margs, output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr\n\u001b[1;32m   1215\u001b[0m                 )\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m-> 1218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(exc\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blocking:\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstatus_method\u001b[39m():\n",
      "\u001b[0;31mOSError\u001b[0m: batch response: Authorization error.\nerror: failed to push some refs to 'https://huggingface.co/natedog/my_awesome_billsum_model'\n"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3f6a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"/Users/nateweinstein/Projects/hugging_face/intro/my_awesome_billsum_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "601b506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text='There’s a growing appetite for alternative search engines. At least, that’s the crux of the argument Richard Socher, the former chief scientist at Salesforce, likes to make. In 2020, Socher co-founded You, a search engine that uses AI to understand search queries, rank the results and parse the queries into different languages (including programming languages). You summarizes information from across the web and offers built-in apps, like search tools for Twitter, that allow users to complete tasks without having to leave the results page. It seems there’s some truth to his words. Socher claims that You has hundreds of thousands of users, with 70% growth in sign-ups last month and 30% growth in unique searches month over month. While that pales in comparison to the world’s most popular search engines (i.e., Google, Bing), which have hundreds of millions of users, Socher draws attention to You’s retention rate. Fifty percent of people who set You as their default search engine continue to use it after the fact. The numbers are to investors’ liking. Today, You closed a $25 million funding round led by Radical Ventures with participation from Salesforce CEO Marc Benioff’s Time Ventures, Breyer Capital, Norwest Venture Partners and Day One Ventures. It brings the startup’s total raised to $45 million, which Socher said will be put toward developing “premium features” and collaborating with outside developers to “show more useful actionable apps” in You’s search results page. “We envision You.com becoming a search platform that is open and allows others to build on top of all of the search technology that we’ve created. Data transparency, user customization, summarization, privacy and state of the art search are the foundation of our platform, and we beat Google in the long run by empowering the world to build the next search experience together,” Socher told TechCrunch in an email interview. “Google is a monolithic, monopolistic search engine that is closed and has ultimately weaponized AI against users for the sake of serving its true purpose: advertising. We are building You.com as a search platform that is open and emphasizes directly serving user needs with You.com apps instead of bombarding people with ads.” Socher previously founded MetaMind, an AI startup that was acquired by Salesforce in 2016. While at Salesforce, Socher helped to build the company’s sprawling Einstein AI platform and worked with Bryan McCann, You’s second co-founder, on natural language processing research.   Unsurprisingly given Socher’s data science background, You sprinkles AI-enabled features liberally throughout the search experience. A capability newly launched today, YouCode can generate code along the lines of GitHub’s Copilot based on a search query. And the recently debuted YouWrite, powered by OpenxAI’s GPT-3, can be prompted to write essays, blog posts and even boilerplate letters.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "128c2a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: nltk in ./.env/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.env/lib/python3.9/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in ./.env/lib/python3.9/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in ./.env/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in ./.env/lib/python3.9/site-packages (from nltk) (8.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nateweinstein/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76382e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', '’', 's', 'a', 'growing', 'appetite', 'for', 'alternative', 'search', 'engines', '.', 'At', 'least', ',', 'that', '’', 's', 'the', 'crux', 'of', 'the', 'argument', 'Richard', 'Socher', ',', 'the', 'former', 'chief', 'scientist', 'at', 'Salesforce', ',', 'likes', 'to', 'make', '.', 'In', '2020', ',', 'Socher', 'co-founded', 'You', ',', 'a', 'search', 'engine', 'that', 'uses', 'AI', 'to', 'understand', 'search', 'queries', ',', 'rank', 'the', 'results', 'and', 'parse', 'the', 'queries', 'into', 'different', 'languages', '(', 'including', 'programming', 'languages', ')', '.', 'You', 'summarizes', 'information', 'from', 'across', 'the', 'web', 'and', 'offers', 'built-in', 'apps', ',', 'like', 'search', 'tools', 'for', 'Twitter', ',', 'that', 'allow', 'users', 'to', 'complete', 'tasks', 'without', 'having', 'to', 'leave', 'the', 'results', 'page', '.', 'It', 'seems', 'there', '’', 's', 'some', 'truth', 'to', 'his', 'words', '.', 'Socher', 'claims', 'that', 'You', 'has', 'hundreds', 'of', 'thousands', 'of', 'users', ',', 'with', '70', '%', 'growth', 'in', 'sign-ups', 'last', 'month', 'and', '30', '%', 'growth', 'in', 'unique', 'searches', 'month', 'over', 'month', '.', 'While', 'that', 'pales', 'in', 'comparison', 'to', 'the', 'world', '’', 's', 'most', 'popular', 'search', 'engines', '(', 'i.e.', ',', 'Google', ',', 'Bing', ')', ',', 'which', 'have', 'hundreds', 'of', 'millions', 'of', 'users', ',', 'Socher', 'draws', 'attention', 'to', 'You', '’', 's', 'retention', 'rate', '.', 'Fifty', 'percent', 'of', 'people', 'who', 'set', 'You', 'as', 'their', 'default', 'search', 'engine', 'continue', 'to', 'use', 'it', 'after', 'the', 'fact', '.', 'The', 'numbers', 'are', 'to', 'investors', '’', 'liking', '.', 'Today', ',', 'You', 'closed', 'a', '$', '25', 'million', 'funding', 'round', 'led', 'by', 'Radical', 'Ventures', 'with', 'participation', 'from', 'Salesforce', 'CEO', 'Marc', 'Benioff', '’', 's', 'Time', 'Ventures', ',', 'Breyer', 'Capital', ',', 'Norwest', 'Venture', 'Partners', 'and', 'Day', 'One', 'Ventures', '.', 'It', 'brings', 'the', 'startup', '’', 's', 'total', 'raised', 'to', '$', '45', 'million', ',', 'which', 'Socher', 'said', 'will', 'be', 'put', 'toward', 'developing', '“', 'premium', 'features', '”', 'and', 'collaborating', 'with', 'outside', 'developers', 'to', '“', 'show', 'more', 'useful', 'actionable', 'apps', '”', 'in', 'You', '’', 's', 'search', 'results', 'page', '.', '“', 'We', 'envision', 'You.com', 'becoming', 'a', 'search', 'platform', 'that', 'is', 'open', 'and', 'allows', 'others', 'to', 'build', 'on', 'top', 'of', 'all', 'of', 'the', 'search', 'technology', 'that', 'we', '’', 've', 'created', '.', 'Data', 'transparency', ',', 'user', 'customization', ',', 'summarization', ',', 'privacy', 'and', 'state', 'of', 'the', 'art', 'search', 'are', 'the', 'foundation', 'of', 'our', 'platform', ',', 'and', 'we', 'beat', 'Google', 'in', 'the', 'long', 'run', 'by', 'empowering', 'the', 'world', 'to', 'build', 'the', 'next', 'search', 'experience', 'together', ',', '”', 'Socher', 'told', 'TechCrunch', 'in', 'an', 'email', 'interview', '.', '“', 'Google', 'is', 'a', 'monolithic', ',', 'monopolistic', 'search', 'engine', 'that', 'is', 'closed', 'and', 'has', 'ultimately', 'weaponized', 'AI', 'against', 'users', 'for', 'the', 'sake', 'of', 'serving', 'its', 'true', 'purpose', ':', 'advertising', '.', 'We', 'are', 'building', 'You.com', 'as', 'a', 'search', 'platform', 'that', 'is', 'open', 'and', 'emphasizes', 'directly', 'serving', 'user', 'needs', 'with', 'You.com', 'apps', 'instead', 'of', 'bombarding', 'people', 'with', 'ads.', '”', 'Socher', 'previously', 'founded', 'MetaMind', ',', 'an', 'AI', 'startup', 'that', 'was', 'acquired', 'by', 'Salesforce', 'in', '2016', '.', 'While', 'at', 'Salesforce', ',', 'Socher', 'helped', 'to', 'build', 'the', 'company', '’', 's', 'sprawling', 'Einstein', 'AI', 'platform', 'and', 'worked', 'with', 'Bryan', 'McCann', ',', 'You', '’', 's', 'second', 'co-founder', ',', 'on', 'natural', 'language', 'processing', 'research', '.', 'Unsurprisingly', 'given', 'Socher', '’', 's', 'data', 'science', 'background', ',', 'You', 'sprinkles', 'AI-enabled', 'features', 'liberally', 'throughout', 'the', 'search', 'experience', '.', 'A', 'capability', 'newly', 'launched', 'today', ',', 'YouCode', 'can', 'generate', 'code', 'along', 'the', 'lines', 'of', 'GitHub', '’', 's', 'Copilot', 'based', 'on', 'a', 'search', 'query', '.', 'And', 'the', 'recently', 'debuted', 'YouWrite', ',', 'powered', 'by', 'OpenAI', '’', 's', 'GPT-3', ',', 'can', 'be', 'prompted', 'to', 'write', 'essays', ',', 'blog', 'posts', 'and', 'even', 'boilerplate', 'letters', '.']\n",
      "Number of Words:  552\n"
     ]
    }
   ],
   "source": [
    "# nltk_tokens = nltk.word_tokenize(text)\n",
    "# print(nltk_tokens)\n",
    "# print(\"Number of Words: \" , len(nltk_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7edf8305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinydb import TinyDB, Query\n",
    "from lib.get_web import strip_tags\n",
    "\n",
    "# Set up a temp DB for easier writing\n",
    "db = TinyDB('db/database.json')\n",
    "\n",
    "def search_word_in_string(word, string, size):\n",
    "    index = string.find(word)\n",
    "    if index == -1:\n",
    "        return \"Word not found in the string.\"\n",
    "    start = max(0, index - size // 2)\n",
    "    end = min(len(string), start + size)\n",
    "    return string[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ab62966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but you input_length is only 3. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing... Using 0 articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'a spokesman for the ssnns ffnn f fnr fps .'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author = 'Chris Dixon'\n",
    "term = 'climate'\n",
    "\n",
    "entry = Query()\n",
    "authorArticles = db.search(entry.author.matches('.*'+author+'.*') & entry.body.search('.*'+term+'.*'))\n",
    "relevant = ''\n",
    "\n",
    "for art in authorArticles:\n",
    "    cleaned = strip_tags(art['body'])\n",
    "    relevant = relevant +' '+ search_word_in_string(term, cleaned, 7000)\n",
    "print(\"Summarizing... Using \"+str(len(authorArticles))+' articles')\n",
    "summarizer(relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0aa359f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'In 2020, Socher co-founded You, a search engine that uses AI to understand search queries, rank the results and parse the queries into different languages (including programming languages) You has hundreds of thousands of users, with 70% growth in sign-ups last month and 30% growth in unique searches month over month. You closed a $25 million funding round with participation from Salesforce CEO Marc Benioff’s Time Ventures, Breyer Capital, Norwest Venture Partners and Day One Ventures.'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
